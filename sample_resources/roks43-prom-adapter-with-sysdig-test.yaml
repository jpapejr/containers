apiVersion: v1
kind: List
metadata:
  name: roks-prom-adapter-test-yaml
items:
  - #project setup
    apiVersion: project.openshift.io/v1
    kind: Project
    metadata:
      name: metrics-test
      labels:
        razee/watch-resource: "lite"
    spec: {}
  - apiVersion: project.openshift.io/v1
    kind: Project
    metadata:
      name: app1
      labels:
        razee/watch-resource: "lite"
    spec: {}
  - apiVersion: project.openshift.io/v1
    kind: Project
    metadata:
      name: app2
      labels:
        razee/watch-resource: "lite"
    spec: {}
  - # prom operator setup
    apiVersion: operators.coreos.com/v1
    kind: OperatorGroup
    metadata:
      annotations:
        olm.providedAPIs: >-
          Alertmanager.v1.monitoring.coreos.com,PodMonitor.v1.monitoring.coreos.com,Prometheus.v1.monitoring.coreos.com,PrometheusRule.v1.monitoring.coreos.com,ServiceMonitor.v1.monitoring.coreos.com
      name: default
      namespace: metrics-test
      labels:
        razee/watch-resource: "lite"
    spec:
      targetNamespaces:
        - metrics-test
  - apiVersion: operators.coreos.com/v1alpha1
    kind: Subscription
    metadata:
      name: prometheus
      namespace: metrics-test
      labels:
        razee/watch-resource: "lite"
    spec:
      channel: beta
      installPlanApproval: Automatic
      name: prometheus
      source: community-operators
      sourceNamespace: openshift-marketplace
      startingCSV: prometheusoperator.0.37.0
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: prom-k8s-admin
      labels:
        razee/watch-resource: "lite"
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
      - kind: ServiceAccount
        name: prometheus-k8s
        namespace: metrics-test
  - # stub a route for the prom service - hostname needs updating
    kind: Route
    apiVersion: route.openshift.io/v1
    metadata:
      name: prom
      namespace: metrics-test
      labels:
        operated-prometheus: "true"
        razee/watch-resource: "lite"
      annotations:
        openshift.io/host.generated: "true"
    spec:
      host: >-
        prom-metrics-test.52.117.75.197.nip.io
      to:
        kind: Service
        name: prometheus-operated
        weight: 100
      port:
        targetPort: web
      wildcardPolicy: None
  - #prom instance
    apiVersion: monitoring.coreos.com/v1
    kind: Prometheus
    metadata:
      labels:
        prometheus: k8s
        razee/watch-resource: "lite"
      name: example
      namespace: metrics-test
    spec:
      alerting:
        alertmanagers:
          - name: alertmanager-main
            namespace: monitoring
            port: web
      replicas: 2
      ruleSelector: {}
      securityContext: {}
      serviceAccountName: prometheus-k8s
      serviceMonitorSelector:
        matchLabels:
          app: instrumented
  - # monitoring config
    apiVersion: monitoring.coreos.com/v1
    kind: ServiceMonitor
    metadata:
      labels:
        app: instrumented
        k8s-app: prometheus
        razee/watch-resource: "lite"
      name: example
      namespace: metrics-test
    spec:
      endpoints:
        - interval: 10s
          port: 8080-tcp
      namespaceSelector:
        matchNames:
          - app1
          - app2
      selector:
        matchLabels:
          app: instrumented
  - #sample app 1
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      annotations:
        alpha.image.policy.openshift.io/resolve-names: "*"
        deployment.kubernetes.io/revision: "2"
        image.openshift.io/triggers: >-
          [{"from":{"kind":"ImageStreamTag","name":"instrumented:latest","namespace":"default"},"fieldPath":"spec.template.spec.containers[?(@.name==\"instrumented\")].image"}]
        openshift.io/generated-by: OpenShiftWebConsole
      name: instrumented
      namespace: app1
      labels:
        app: instrumented
        app.kubernetes.io/component: instrumented
        app.kubernetes.io/instance: instrumented
        app.kubernetes.io/part-of: instrumented-app
        razee/watch-resource: "lite"
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: instrumented
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: instrumented
            deploymentconfig: instrumented
            razee/watch-resource: "lite"
          annotations:
            openshift.io/generated-by: OpenShiftWebConsole
            prometheus.io/scrape: "true"
            prometheus.io/port: "8080"
        spec:
          containers:
            - name: instrumented
              image: fabxc/instrumented_app
              ports:
                - containerPort: 8080
                  protocol: TCP
              resources: {}
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              imagePullPolicy: Always
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 25%
          maxSurge: 25%
      revisionHistoryLimit: 10
      progressDeadlineSeconds: 600
  - kind: Service
    apiVersion: v1
    metadata:
      name: instrumented
      namespace: app1
      labels:
        app: instrumented
        app.kubernetes.io/component: instrumented
        app.kubernetes.io/instance: instrumented
        app.kubernetes.io/name: ""
        app.kubernetes.io/part-of: instrumented-app
        app.openshift.io/runtime: ""
        app.openshift.io/runtime-version: latest
        razee/watch-resource: "lite"
    spec:
      ports:
        - name: 8080-tcp
          protocol: TCP
          port: 8080
          targetPort: 8080
      selector:
        app: instrumented
        deploymentconfig: instrumented
      type: ClusterIP
      sessionAffinity: None
  - kind: Route
    apiVersion: route.openshift.io/v1
    metadata:
      name: instrumented
      namespace: app1
      labels:
        app: instrumented
        app.kubernetes.io/component: instrumented
        app.kubernetes.io/instance: instrumented
        app.kubernetes.io/name: ""
        app.kubernetes.io/part-of: instrumented-app
        app.openshift.io/runtime: ""
        app.openshift.io/runtime-version: latest
        razee/watch-resource: "lite"
      annotations:
        openshift.io/host.generated: "true"
    spec:
      host: instrumented-app1.52.117.75.197.nip.io
      to:
        kind: Service
        name: instrumented
        weight: 100
      port:
        targetPort: 8080-tcp
      wildcardPolicy: None
  - # sample app 2
    kind: Deployment
    apiVersion: apps/v1
    metadata:
      annotations:
        alpha.image.policy.openshift.io/resolve-names: "*"
        deployment.kubernetes.io/revision: "2"
        image.openshift.io/triggers: >-
          [{"from":{"kind":"ImageStreamTag","name":"instrumented:latest","namespace":"default"},"fieldPath":"spec.template.spec.containers[?(@.name==\"instrumented\")].image"}]
        openshift.io/generated-by: OpenShiftWebConsole
      name: instrumented
      namespace: app2
      labels:
        app: instrumented
        app.kubernetes.io/component: instrumented
        app.kubernetes.io/instance: instrumented
        app.kubernetes.io/part-of: instrumented-app
        razee/watch-resource: "lite"
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: instrumented
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: instrumented
            deploymentconfig: instrumented
            razee/watch-resource: "lite"
          annotations:
            openshift.io/generated-by: OpenShiftWebConsole
            prometheus.io/scrape: "true"
            prometheus.io/port: "8080"
        spec:
          containers:
            - name: instrumented
              image: fabxc/instrumented_app
              ports:
                - containerPort: 8080
                  protocol: TCP
              resources: {}
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              imagePullPolicy: Always
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
          dnsPolicy: ClusterFirst
          securityContext: {}
          schedulerName: default-scheduler
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 25%
          maxSurge: 25%
      revisionHistoryLimit: 10
      progressDeadlineSeconds: 600
  - kind: Service
    apiVersion: v1
    metadata:
      name: instrumented
      namespace: app2
      labels:
        app: instrumented
        app.kubernetes.io/component: instrumented
        app.kubernetes.io/instance: instrumented
        app.kubernetes.io/name: ""
        app.kubernetes.io/part-of: instrumented-app
        app.openshift.io/runtime: ""
        app.openshift.io/runtime-version: latest
        razee/watch-resource: "lite"
    spec:
      ports:
        - name: 8080-tcp
          protocol: TCP
          port: 8080
          targetPort: 8080
      selector:
        app: instrumented
        deploymentconfig: instrumented
      type: ClusterIP
      sessionAffinity: None
  - kind: Route
    apiVersion: route.openshift.io/v1
    metadata:
      name: instrumented
      namespace: app2
      labels:
        app: instrumented
        app.kubernetes.io/component: instrumented
        app.kubernetes.io/instance: instrumented
        app.kubernetes.io/name: ""
        app.kubernetes.io/part-of: instrumented-app
        app.openshift.io/runtime: ""
        app.openshift.io/runtime-version: latest
        razee/watch-resource: "lite"
      annotations:
        openshift.io/host.generated: "true"
    spec:
      host: instrumented-app2.52.117.75.197.nip.io
      to:
        kind: Service
        name: instrumented
        weight: 100
      port:
        targetPort: 8080-tcp
      wildcardPolicy: None
  - #prom-adapter rbac
    kind: ServiceAccount
    apiVersion: v1
    metadata:
      name: custom-metrics-apiserver
      namespace: default
      labels:
        razee/watch-resource: "lite"
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: custom-metrics-server-resources
      labels:
        razee/watch-resource: "lite"
    rules:
      - apiGroups:
          - custom.metrics.k8s.io
        resources: ["*"]
        verbs: ["*"]
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: custom-metrics-resource-reader
      labels:
        razee/watch-resource: "lite"
    rules:
      - apiGroups:
          - ""
        resources:
          - namespaces
          - pods
          - services
        verbs:
          - get
          - list
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: custom-metrics:system:auth-delegator
      labels:
        razee/watch-resource: "lite"
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: system:auth-delegator
    subjects:
      - kind: ServiceAccount
        name: custom-metrics-apiserver
        namespace: default
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: custom-metrics-auth-reader
      namespace: kube-system
      labels:
        razee/watch-resource: "lite"
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: extension-apiserver-authentication-reader
    subjects:
      - kind: ServiceAccount
        name: custom-metrics-apiserver
        namespace: default
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: custom-metrics-resource-reader
      labels:
        razee/watch-resource: "lite"
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: custom-metrics-resource-reader
    subjects:
      - kind: ServiceAccount
        name: custom-metrics-apiserver
        namespace: default
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: hpa-controller-custom-metrics
      labels:
        razee/watch-resource: "lite"
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: custom-metrics-server-resources
    subjects:
      - kind: ServiceAccount
        name: horizontal-pod-autoscaler
        namespace: kube-system
  - #metric config
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: adapter-config
      namespace: default
      labels:
        razee/watch-resource: "debug"
    data:
      config.yaml: |
        rules:
        - seriesQuery: 'codelab_api_http_requests_in_progress{service="instrumented"}'
          resources:
            overrides:
              namespace: {resource: "namespace"}
              pod: {resource: "pod"}
              service: {resource: "service"}
          name:
            matches: "^codelab_api_(.*)_in_progress"
            as: "${1}_concurrent"
          metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>)'
  - #register the prom-adapter with APIService
    apiVersion: v1
    kind: Service
    metadata:
      annotations:
        service.alpha.openshift.io/serving-cert-secret-name: prometheus-adapter-tls
      labels:
        name: prometheus-adapter
        razee/watch-resource: "lite"
      name: prometheus-adapter
      namespace: default
    spec:
      ports:
        - name: https
          port: 443
          targetPort: 6443
      selector:
        app: prometheus-adapter
      type: ClusterIP
  - apiVersion: apiregistration.k8s.io/v1beta1
    kind: APIService
    metadata:
      name: v1beta1.custom.metrics.k8s.io
      labels:
        razee/watch-resource: "lite"
    spec:
      service:
        name: prometheus-adapter
        namespace: default
      group: custom.metrics.k8s.io
      version: v1beta1
      insecureSkipTLSVerify: true
      groupPriorityMinimum: 100
      versionPriority: 100
  - # deploy the prom-adapter
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: prometheus-adapter
        razee/watch-resource: "lite"
      name: prometheus-adapter
      namespace: default
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: prometheus-adapter
      template:
        metadata:
          labels:
            app: prometheus-adapter
            razee/watch-resource: "lite"
          name: prometheus-adapter
        spec:
          serviceAccountName: custom-metrics-apiserver
          containers:
            - name: prometheus-adapter
              image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0186a59ee7a43765f28ce48e4ddc5492561a093bbf12fb9ab1c8afb14eefbdcf
              args:
                - --secure-port=6443
                - --tls-cert-file=/var/run/serving-cert/tls.crt
                - --tls-private-key-file=/var/run/serving-cert/tls.key
                - --logtostderr=true
                - --prometheus-url=http://prometheus-operated.metrics-test.svc:9090/
                - --prometheus-token-file=/etc/adapter/token.txt
                - --metrics-relist-interval=1m
                - --v=4
                - --config=/etc/adapter/config.yaml
              ports:
                - containerPort: 6443
              volumeMounts:
                - mountPath: /var/run/serving-cert
                  name: volume-serving-cert
                  readOnly: true
                - mountPath: /etc/adapter/
                  name: config
                  readOnly: true
                - mountPath: /tmp
                  name: tmp-vol
                - mountPath: /etc/adapter/
                  name: sysdig-token
                  readOnly: true
          volumes:
            - name: volume-serving-cert
              secret:
                secretName: prometheus-adapter-tls
            - name: sysdig-token
              secret:
                secretName: sysdig-token
            - name: config
              configMap:
                name: adapter-config
            - name: tmp-vol
              emptyDir: {}
  - # set up HPA for app1
    apiVersion: autoscaling/v2beta1
    kind: HorizontalPodAutoscaler
    metadata:
      name: instrumented-autoscaler
      namespace: app1
      labels:
        razee/watch-resource: "lite"
    spec:
      scaleTargetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: instrumented
      minReplicas: 1
      maxReplicas: 10
      metrics:
        - type: Object
          object:
            target:
              kind: Service
              name: instrumented
            metricName: http_requests_concurrent
            targetValue: '5'
  - # set up HPA for app2
    apiVersion: autoscaling/v2beta1
    kind: HorizontalPodAutoscaler
    metadata:
      name: instrumented-autoscaler
      namespace: app2
      labels:
        razee/watch-resource: "lite"
    spec:
      scaleTargetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: instrumented
      minReplicas: 1
      maxReplicas: 7
      metrics:
        - type: Object
          object:
            target:
              kind: Service
              name: instrumented
            metricName: http_requests_concurrent
            targetValue: '5'
